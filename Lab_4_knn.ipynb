{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_4_knn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# K-Nearest Neighbor Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. (40%) Implement the k-nearest neighbor algorithm and the k-nearest neighbor regression algorithm, including optional distance weighting for both algorithms. \n",
        "\n",
        "### Code requirements\n",
        "- Use Euclidean distance to decide closest neighbors. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a2KSZ_7AN0G"
      },
      "source": [
        "class KNNClassifier(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "\n",
        "    def __init__(self, columntype=[], weight_type='inverse_distance', k=5): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            columntype for each column tells you if continues[real] or if nominal[categoritcal].\n",
        "            weight_type: inverse_distance voting or if non distance weighting. Options = [\"no_weight\",\"inverse_distance\"]\n",
        "        \"\"\"\n",
        "        self.columntype = columntype #Note This won't be needed until part 5\n",
        "        self.weight_type = weight_type\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, data, labels):\n",
        "        \"\"\" Fit the data; run the algorithm (for this lab really just saves the data :D)\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "            y (array-like): A 2D numpy array with the training targets\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        return self\n",
        "    \n",
        "    def predict(self, data):\n",
        "        \"\"\" Predict all classes for a dataset X\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "\n",
        "        predict_array = []\n",
        "        for i, i_point in enumerate(data):\n",
        "          distances = []\n",
        "          for j, j_point in enumerate(self.data):\n",
        "            distances.append(((np.linalg.norm(j_point - i_point)), self.labels[j]))\n",
        "          distances.sort()\n",
        "          neighbors = distances[:self.k]\n",
        "\n",
        "          target_classes = np.unique(self.labels)\n",
        "          votes = dict.fromkeys(target_classes, 0)\n",
        "          if self.weight_type == 'inverse_distance':\n",
        "            for n in neighbors:\n",
        "              votes[n[1]] += 1/n[0]**2 # Vote is weighted by distance\n",
        "          else:\n",
        "            for n in neighbors:\n",
        "              votes[n[1]] += 1\n",
        "          predict_class = max(votes, key=votes.get)\n",
        "          predict_array.append(predict_class)\n",
        "\n",
        "        return predict_array\n",
        "\n",
        "    #Returns the Mean score given input data and labels\n",
        "    def score(self, X, y):\n",
        "        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 2D numpy array with targets\n",
        "        Returns:\n",
        "            score : float\n",
        "                Mean accuracy of self.predict(X) wrt. y.\n",
        "        \"\"\"\n",
        "        count = 0\n",
        "        predictions = self.predict(X)\n",
        "        for i in range(len(predictions)):\n",
        "          if predictions[i] == y[i]:\n",
        "            count += 1\n",
        "        mean = count / len(predictions)\n",
        "\n",
        "        return mean\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FERUhGAAJh4j"
      },
      "source": [
        "## 1.1 Debug\n",
        "Debug your model by running it on the [seismic bumps](https://archive.ics.uci.edu/ml/datasets/seismic-bumps) problem.\n",
        "- Use this [training set](https://github.com/cs472ta/CS472/blob/master/datasets/seismic-bumps_train.arff) and this [test set](https://github.com/cs472ta/CS472/blob/master/datasets/seismic-bumps_test.arff)\n",
        "- Use distance weighting\n",
        "- KNN = 3 (three nearest neighbors)\n",
        "- Donâ€™t normalize the data\n",
        "- Use Euclidean Distance\n",
        "\n",
        "---\n",
        "\n",
        "Expected Results:\n",
        "- Acc = [93.57]\n",
        "- Link to [debug solution](https://github.com/cs472ta/CS472/blob/master/debug_solutions/seismic-bump-prediction.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QSWFAASJh4k",
        "outputId": "ecba4a23-e965-4206-cdf0-b54e9eaae13a"
      },
      "source": [
        "# Load seismic bumps data\n",
        "\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/seismic-bumps_train.arff --output seismic-train.arff\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/seismic-bumps_test.arff --output seismic-test.arff\n",
        "\n",
        "seismic_train = arff.loadarff('seismic-train.arff')\n",
        "seismic_test = arff.loadarff('seismic-test.arff')\n",
        "\n",
        "seismic_train = pd.DataFrame(seismic_train[0])\n",
        "seismic_test = pd.DataFrame(seismic_test[0])\n",
        "\n",
        "seismic_train['Class'] = [int(s.decode()) for s in seismic_train['Class']]\n",
        "seismic_test['Class'] = [int(s.decode()) for s in seismic_test['Class']]\n",
        "\n",
        "seismic_train.head()\n",
        "\n",
        "seismic_train = np.array(seismic_train)\n",
        "seismic_train_data = seismic_train[:,:-1]\n",
        "seismic_train_targets = seismic_train[:,-1]\n",
        "\n",
        "seismic_test = np.array(seismic_test)\n",
        "seismic_test_data = seismic_test[:,:-1]\n",
        "seismic_test_targets = seismic_test[:,-1]\n",
        "\n",
        "# Train on training set\n",
        "\n",
        "my_knn = KNNClassifier(weight_type='no_weight', k=3)\n",
        "my_knn.fit(seismic_train_data, seismic_train_targets)\n",
        "\n",
        "# Predict on test set\n",
        "\n",
        "my_knn.predict(seismic_test_data)\n",
        "my_knn.score(seismic_test_data, seismic_test_targets)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6392  100  6392    0     0   122k      0 --:--:-- --:--:-- --:--:--  120k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  9597  100  9597    0     0   153k      0 --:--:-- --:--:-- --:--:--  153k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9357142857142857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2. (10%) Use the k-nearest neighbor algorithm (without distance weighting) for the [magic telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope) problem\n",
        "\n",
        "- Use this [training set](https://github.com/cs472ta/CS472/blob/master/datasets/magic_telescope_train.arff) and this [test set](https://github.com/cs472ta/CS472/blob/master/datasets/magic_telescope_test.arff) \n",
        "\n",
        "## 2.1\n",
        "- Try it with k=3 with normalization (input features normalized between 0 and 1) and without normalization and discuss the accuracy results on the test set. Use the formula (x-xmin)/(xmax-xmin)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SSoasDQSKXb"
      },
      "source": [
        "# Load magic telescope data\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_train.arff --output magic_telescope_train.arff\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_test.arff --output magic_telescope_test.arff\n",
        "\n",
        "train_data = arff.loadarff('magic_telescope_train.arff')\n",
        "test_data = arff.loadarff('magic_telescope_test.arff')\n",
        "\n",
        "magic_train_df = pd.DataFrame(train_data[0])\n",
        "magic_test_df = pd.DataFrame(test_data[0])\n",
        "\n",
        "magic_train_df['class'] = magic_train_df['class'].str.decode('utf-8')\n",
        "magic_test_df['class'] = magic_test_df['class'].str.decode('utf-8')\n",
        "\n",
        "magic_train = np.array(magic_train_df)\n",
        "magic_train_data = magic_train[:,:-1]\n",
        "magic_train_targets = magic_train[:,-1]\n",
        "\n",
        "magic_test = np.array(magic_test_df)\n",
        "magic_test_data = magic_test[:,:-1]\n",
        "magic_test_targets = magic_test[:,-1]\n",
        "\n",
        "maximums = np.amax(magic_train_data, axis=0)\n",
        "minimums = np.amin(magic_train_data, axis=0)\n",
        "  \n",
        "magic_test_data_normalized = np.zeros_like(magic_test_data)\n",
        "\n",
        "for i in range(len(magic_test_data[0])):\n",
        "  for j in range(len(magic_test_data)):\n",
        "    magic_test_data_normalized[j][i] = (magic_test_data[j][i] - minimums[i]) / (maximums[i] - minimums[i])\n",
        "\n",
        "# Train/Predict with normalization\n",
        "\n",
        "magic_knn = KNNClassifier(k=3)\n",
        "magic_knn.fit(magic_train_data, magic_train_targets)\n",
        "norm_score = magic_knn.score(magic_test_data_normalized, magic_test_targets)\n",
        "print(norm_score)\n",
        "\n",
        "# Train/Predict without normalization\n",
        "\n",
        "without_norm_score = magic_knn.score(magic_test_data, magic_test_targets)\n",
        "print(without_norm_score)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wINDm4XjJh4n"
      },
      "source": [
        "*Discuss the accuracy results*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiGDDlI9Jh4n"
      },
      "source": [
        "## 2.2\n",
        "\n",
        "- With just the normalized training set as your data, graph classification accuracy on the test set with odd values of k from 1 to 15.\n",
        "    - Which value of k is the best in terms of classification accuracy?\n",
        "    - As a rough sanity check, typical knn accuracies for the magic telescope data set are 75-85%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-h3-2VCJh4o"
      },
      "source": [
        "# Train/Predict with normalization using k=1,3,...,15\n",
        "\n",
        "# Graph classification accuracy over k\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrwUd1YLJh4o"
      },
      "source": [
        "# For the rest of the experiments use only normalized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "## 3. (10%) Use the regression variation of your algorithm (without distance weighting) for the [housing price prediction](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) problem.\n",
        "\n",
        "- Use this [training set](https://github.com/cs472ta/CS472/blob/master/datasets/housing_train.arff) and this [test set](https://github.com/cs472ta/CS472/blob/master/datasets/housing_test.arff).\n",
        "- Use Mean Square Error (MSE) on the test set as your accuracy metric for this case.\n",
        "    - Do not normalize regression output values\n",
        "- Graph MSE on the test set with odd values of k from 1 to 15\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBGUn43ASiXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfdcfb8-3bf7-428f-bd7f-129f53694035"
      },
      "source": [
        "# Load housing price prediction data\n",
        "\n",
        "# Download file with curl\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_train.arff --output housing-train.arff\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_test.arff --output housing-test.arff\n",
        "\n",
        "housing_train = arff.loadarff('housing-train.arff')\n",
        "housing_test = arff.loadarff('housing-test.arff')\n",
        "\n",
        "housing_train = pd.DataFrame(housing_train[0])\n",
        "housing_test = pd.DataFrame(housing_test[0])\n",
        "\n",
        "housing_train.CHAS = [int(s.decode()) for s in housing_train.CHAS]\n",
        "housing_test.CHAS = [int(s.decode()) for s in housing_test.CHAS]\n",
        "\n",
        "print(housing_train)\n",
        "\n",
        "housing_train = np.array(housing_train)\n",
        "housing_train_data = housing_train[:,:-1]\n",
        "housing_train_targets = housing_train[:,-1]\n",
        "\n",
        "housing_test = np.array(housing_test)\n",
        "housing_test_data = housing_test[:,:-1]\n",
        "housing_test_targets = housing_test[:,-1]\n",
        "\n",
        "# Train/Predict using k=1,3,...,15\n",
        "\n",
        "# Graph MSE over k\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 34205  100 34205    0     0   263k      0 --:--:-- --:--:-- --:--:--  265k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3885  100  3885    0     0  34078      0 --:--:-- --:--:-- --:--:-- 33782\n",
            "        CRIM   ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  MEDV\n",
            "0    0.02731  0.0   7.07     0  0.469  ...  242.0     17.8  396.90   9.14  21.6\n",
            "1    0.02729  0.0   7.07     0  0.469  ...  242.0     17.8  392.83   4.03  34.7\n",
            "2    0.03237  0.0   2.18     0  0.458  ...  222.0     18.7  394.63   2.94  33.4\n",
            "3    0.06905  0.0   2.18     0  0.458  ...  222.0     18.7  396.90   5.33  36.2\n",
            "4    0.02985  0.0   2.18     0  0.458  ...  222.0     18.7  394.12   5.21  28.7\n",
            "..       ...  ...    ...   ...    ...  ...    ...      ...     ...    ...   ...\n",
            "450  0.06263  0.0  11.93     0  0.573  ...  273.0     21.0  391.99   9.67  22.4\n",
            "451  0.04527  0.0  11.93     0  0.573  ...  273.0     21.0  396.90   9.08  20.6\n",
            "452  0.06076  0.0  11.93     0  0.573  ...  273.0     21.0  396.90   5.64  23.9\n",
            "453  0.10959  0.0  11.93     0  0.573  ...  273.0     21.0  393.45   6.48  22.0\n",
            "454  0.04741  0.0  11.93     0  0.573  ...  273.0     21.0  396.90   7.88  11.9\n",
            "\n",
            "[455 rows x 14 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v19fpixqTe-7"
      },
      "source": [
        "## 4. (15%) Repeat your experiments for magic telescope and housing using distance-weighted (inverse of distance squared) voting and discuss your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCPFUAGTS2sX"
      },
      "source": [
        "# Train/Predict magic telescope using distance-weighted voting\n",
        "\n",
        "# Train/Predict housing using distance-weighted voting\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzM0SCk_Jh4q"
      },
      "source": [
        "*Discuss your results*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVNJbMQDJh4r"
      },
      "source": [
        "## 5. (10%) Use the k-nearest neighbor algorithm to solve the [credit-approval](https://archive.ics.uci.edu/ml/datasets/Credit+Approval) (credit-a) problem.\n",
        "\n",
        "- Use this [dataset](https://github.com/cs472ta/CS472/blob/master/datasets/credit_approval.arff.txt)\n",
        "- Note that this set has both continuous and nominal attributes, together with donâ€™t know values. \n",
        "- Implement and justify a distance metric which supports continuous, nominal, and donâ€™t know attribute values\n",
        "    - You need to handle don't knows with the distance metric, not by imputing a value.\n",
        "    - More information on distance metrics can be found [here](http://axon.cs.byu.edu/~martinez/classes/478/slides/IBL.pdf).\n",
        "- Use your own choice for k, training/test split, etc.\n",
        "- As a rough sanity check, typical k-nn accuracies for the credit data set are 70-80%.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o97fwPpmJh4r"
      },
      "source": [
        "# Load dataset and split into train/test sets\n",
        "\n",
        "# Train/Predict credit-approval\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o23Gl8GoJh4s"
      },
      "source": [
        "*Justify your distance metric*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 6. (15%) Use the scikit's KNN classifier on magic telescope and housing and compare your results.\n",
        "\n",
        "- Try out different hyperparameters to see how well you can do. \n",
        "- Find a data set of your choice (not one previously used) and use the SK version to get results for it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFQv70W2VyqJ"
      },
      "source": [
        "# Train/Predict magic telescope using scikit's KNN\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "magic_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "magic_knn.fit(magic_train_data, magic_train_targets)\n",
        "magic_score = magic_knn.score(magic_test_data, magic_test_targets)\n",
        "print(\"Magic Score: \", magic_score)\n",
        "\n",
        "# Train/Predict housing using scikit's KNN\n",
        "\n",
        "housing_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "housing_knn.fit(housing_train_data, housing_train_targets)\n",
        "housing_score = housing_knn.score(housing_test_data, housing_test_targets)\n",
        "print(\"Housing Score: \", housing_score)\n",
        "\n",
        "# Load a dataset of your choice\n",
        "\n",
        "new_data = pd.read_csv(\"/content/KNNAlgorithmDataset.csv\")\n",
        "new_data = new_data[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
        "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
        "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
        "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
        "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
        "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
        "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
        "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
        "       'symmetry_worst', 'fractal_dimension_worst', 'diagnosis']]\n",
        "\n",
        "new_data = np.array(new_data)\n",
        "new_data_data = new_data[:,:-1]\n",
        "new_data_targets = new_data[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_data_data, new_data_targets, test_size=0.33, random_state=42)\n",
        "\n",
        "# Train/Predict your dataset using scikit's KNN\n",
        "\n",
        "new_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "new_knn.fit(X_train, y_train)\n",
        "new_score = new_knn.score(X_test, y_test)\n",
        "print(\"New Dataset Score: \", new_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Report your comparison*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 7. (optional 5% extra credit): For the best value of k for any one of the datasets, implement a reduction algorithm that removes data points in some rational way such that performance does not drop too drastically on the test set given the reduced training set.\n",
        "\n",
        "- Compare your performance on the test set for the reduced and non-reduced versions and give the number (and percentage) of training examples removed from the original training set. How well does your reduction algorithm work?\n",
        "    - Note that performance for magic telescope is classification accuracy and for housing it is sum squared error.\n",
        "    - Magic Telescope has about 12,000 instances and if you use a leave one out style of testing for your data set reduction, then your algorithm will run slow since that is n2 at each step.\n",
        "    - If you wish, you may use a random subset of 2,000 of the magic telescope instances.\n",
        "    - More information on reduction techniques can be found [here](http://axon.cs.byu.edu/~martinez/classes/478/slides/IBL.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iY77P7gk1Nh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}